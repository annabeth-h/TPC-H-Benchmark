== Physical Plan ==
*(8) HashAggregate(keys=[], functions=[sum(cast(volume#111225 as bigint))])
+- Exchange SinglePartition
   +- *(7) HashAggregate(keys=[], functions=[partial_sum(cast(volume#111225 as bigint))])
      +- *(7) Project [pythonUDF0#125783 AS volume#111225]
         +- BatchEvalPython [<lambda>(l_extendedprice#61, l_discount#62)], [l_discount#62, l_extendedprice#61, pythonUDF0#125783]
            +- *(6) Project [l_discount#62, l_extendedprice#61]
               +- *(6) Project [p_partkey#0, p_brand#3, p_size#5, p_container#6, l_partkey#57, l_quantity#60, l_extendedprice#61, l_discount#62]
                  +- *(6) Filter (((((((p_brand#3 = Brand#12) && pythonUDF0#125780) && (l_quantity#60 >= 1.0)) && (l_quantity#60 <= 11.0)) && (p_size#5 <= 5)) || (((((p_brand#3 = Brand#23) && pythonUDF1#125781) && (l_quantity#60 >= 24.0)) && (l_quantity#60 <= 34.0)) && (p_size#5 <= 15))) || (((((p_brand#3 = Brand#34) && pythonUDF2#125782) && (l_quantity#60 >= 20.0)) && (l_quantity#60 <= 30.0)) && (p_size#5 <= 15)))
                     +- BatchEvalPython [<lambda>(p_container#6), <lambda>(p_container#6), <lambda>(p_container#6)], [p_partkey#0, p_brand#3, p_size#5, p_container#6, l_partkey#57, l_quantity#60, l_extendedprice#61, l_discount#62, pythonUDF0#125780, pythonUDF1#125781, pythonUDF2#125782]
                        +- *(5) SortMergeJoin [p_partkey#0], [l_partkey#57], Inner
                           :- *(2) Sort [p_partkey#0 ASC NULLS FIRST], false, 0
                           :  +- Exchange hashpartitioning(p_partkey#0, 200)
                           :     +- *(1) Project [p_partkey#0, p_brand#3, p_size#5, p_container#6]
                           :        +- *(1) Filter ((isnotnull(p_size#5) && (p_size#5 >= 1)) && isnotnull(p_partkey#0))
                           :           +- *(1) FileScan avro [p_partkey#0,p_brand#3,p_size#5,p_container#6] Batched: false, Format: com.databricks.spark.avro.DefaultSource@754c2a03, Location: InMemoryFileIndex[hdfs://namenode:8020/part.avro], PartitionFilters: [], PushedFilters: [IsNotNull(p_size), GreaterThanOrEqual(p_size,1), IsNotNull(p_partkey)], ReadSchema: struct<p_partkey:int,p_brand:string,p_size:int,p_container:string>
                           +- *(4) Sort [l_partkey#57 ASC NULLS FIRST], false, 0
                              +- Exchange hashpartitioning(l_partkey#57, 200)
                                 +- *(3) Project [l_partkey#57, l_quantity#60, l_extendedprice#61, l_discount#62]
                                    +- *(3) Filter (((isnotnull(l_shipinstruct#69) && ((l_shipmode#70 = AIR) || (l_shipmode#70 = AIR REG))) && (l_shipinstruct#69 = DELIVER IN PERSON)) && isnotnull(l_partkey#57))
                                       +- *(3) FileScan avro [l_partkey#57,l_quantity#60,l_extendedprice#61,l_discount#62,l_shipinstruct#69,l_shipmode#70] Batched: false, Format: com.databricks.spark.avro.DefaultSource@7bfb5a4, Location: InMemoryFileIndex[hdfs://namenode:8020/lineitem.avro], PartitionFilters: [], PushedFilters: [IsNotNull(l_shipinstruct), Or(EqualTo(l_shipmode,AIR),EqualTo(l_shipmode,AIR REG)), EqualTo(l_sh..., ReadSchema: struct<l_partkey:int,l_quantity:double,l_extendedprice:double,l_discount:double,l_shipinstruct:st...
